{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 기본 MNIST(Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#1. 데이터 저장 및 변수 생성\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 28]))\n",
    "b1 = tf.Variable(tf.random_normal([28]))\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2= tf.Variable(tf.random_normal([28, 28]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([28]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3= tf.Variable(tf.random_normal([28, nb_classes]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]), name='bias3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. hypothesis\n",
    "hypothesis = tf.nn.softmax(tf.matmul(layer2, W3) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. 비용함수\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4. optimizer 정의\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5. prediction tensor 정의\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 1.046211348 accuray = 0.800000012\n",
      "Epoch: 0002 cost = 0.553165467 accuray = 0.839999974\n",
      "Epoch: 0003 cost = 0.442879243 accuray = 0.889999986\n",
      "Epoch: 0004 cost = 0.390797881 accuray = 0.899999976\n",
      "Epoch: 0005 cost = 0.354930080 accuray = 0.939999998\n",
      "Epoch: 0006 cost = 0.331254262 accuray = 0.879999995\n",
      "Epoch: 0007 cost = 0.306165370 accuray = 0.939999998\n",
      "Epoch: 0008 cost = 0.291594944 accuray = 0.910000026\n",
      "Epoch: 0009 cost = 0.276178140 accuray = 0.920000017\n",
      "Epoch: 0010 cost = 0.258114714 accuray = 0.939999998\n",
      "Epoch: 0011 cost = 0.257400366 accuray = 0.899999976\n",
      "Epoch: 0012 cost = 0.240179243 accuray = 0.959999979\n",
      "Epoch: 0013 cost = 0.236312562 accuray = 0.860000014\n",
      "Epoch: 0014 cost = 0.229609750 accuray = 0.920000017\n",
      "Epoch: 0015 cost = 0.216622107 accuray = 0.970000029\n",
      "Learning finished\n",
      "Accuracy:  0.927\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1db9d7b128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 6.학습 진행 Training cycle(15회 반복)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        #number of iterations\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "                \n",
    "        #1epoch \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _, a = sess.run([cost, optimizer, accuracy], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost),\n",
    "              'accuray =', '{:.9f}'.format(a))\n",
    "        \n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "        \n",
    "    #시각화\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 Softmax 함수일 때 90%정도 나왔는데 좀 더 좋아졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 기본 MNIST(Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 76s 7us/step\n"
     ]
    }
   ],
   "source": [
    "#1. 데이터 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32')\n",
    "x_test = x_test.reshape(10000, 784).astype('float32')\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. 모델 구성하기(1. 데이터 정의 및 2. 가설 설정)\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation='sigmoid'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. 모델 학습과정 설정하기(3. cost function 4. optimizer 5. accuracy)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 1.0826 - acc: 0.7280\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.5859 - acc: 0.8666: 1s - loss: 0.\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4587 - acc: 0.8901\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.3976 - acc: 0.9012\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.3630 - acc: 0.9071\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.3369 - acc: 0.9122:\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.3155 - acc: 0.9172: 1s - loss: 0.31 - ETA: 0s - loss: 0.3155 - acc: 0\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2999 - acc: 0.9196: 3s  - ETA: 0s - loss: 0.3001\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2865 - acc: 0.9233\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2749 - acc: 0.9257\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.2645 - acc: 0.9287- ETA: 1s - loss: 0.2712 - acc: 0.92 - ETA: 1s - loss: 0. - 3s 47us/step - loss: 0.2645 - acc: 0.9288\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2575 - acc: 0.9294\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2486 - acc: 0.9315\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2407 - acc: 0.9341\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2392 - acc: 0.9351\n"
     ]
    }
   ],
   "source": [
    "#4. 모델 학습시키기(6. train)\n",
    "hist = model.fit(x_train, y_train, epochs=15, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## training loss and acc ##\n",
      "[1.0826497856775921, 0.58585745856165883, 0.45870677635073664, 0.39760554311176138, 0.36302959531545637, 0.33685566668709122, 0.31545800462365148, 0.29985681521395841, 0.28647938765585423, 0.27494910409053169, 0.26445028296361367, 0.25747810501605273, 0.24855611308167377, 0.2407173357779781, 0.23917038244505723]\n",
      "[0.7280333310117324, 0.86664999971787138, 0.89005000064770379, 0.90121666848659521, 0.90711666882038111, 0.91216666907072064, 0.91715000172456107, 0.91963333507378897, 0.92325000266234081, 0.9257000018159548, 0.92883333484331765, 0.9294333339730898, 0.93148333479960754, 0.93406666974226638, 0.93511666923761372]\n"
     ]
    }
   ],
   "source": [
    "#5. 학습과정 살펴보기\n",
    "print('## training loss and acc ##')\n",
    "print(hist.history['loss'])\n",
    "print(hist.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 33us/step\n",
      "## evaluation loss_and_metrics ##\n",
      "[0.23910675685852767, 0.93630000174045558]\n"
     ]
    }
   ],
   "source": [
    "#6. 모델 평가하기(7. model evaluate)\n",
    "loss_and_metircs = model.evaluate(x_test, y_test, batch_size=100)\n",
    "print(\"## evaluation loss_and_metrics ##\")\n",
    "print(loss_and_metircs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## yhat ##\n",
      "[[  5.17563720e-04   1.54603142e-04   9.46832472e-04   1.01054916e-02\n",
      "    8.11101199e-05   2.32238279e-04   2.86299255e-05   9.85582054e-01\n",
      "    6.56548829e-04   1.69491209e-03]]\n"
     ]
    }
   ],
   "source": [
    "#7. 모델 평가하기\n",
    "xhat = x_test[0:1]\n",
    "yhat = model.predict(xhat)\n",
    "print(\"## yhat ##\")\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 는 내가 학습률을 지정해주지 않아도 된다. 그래서 93.63% 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2-1. Upgrade MNIST(Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) RELU 사용<br/>\n",
    "2) AdamOptimizer<br/>\n",
    "3) Weights Initializer Xvaier<br/>\n",
    "4) Dropout<br/>\n",
    "5) Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#1. 데이터 저장 및 변수 생성\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "#dropout을 위한 placeholder 를 따로 둔다. (훈련 / 시험 나누기 위해서)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#Xavier 초기값\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "\n",
    "#Relu 사용(Vanishing Gradient 방지)\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "#dropout layer\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([128]), name='bias3')\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[128, 64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([64]), name='bias4')\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[64, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]), name='bias5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. hypothesis\n",
    "hypothesis = tf.matmul(layer4, W5) + b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4. optimizer(Adam)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./photo/adam.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5. prediction tensor 정의\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "import random, time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.659556384 accuray = 0.889999986\n",
      "Epoch: 0002 cost = 0.456444497 accuray = 0.920000017\n",
      "Epoch: 0003 cost = 0.438435822 accuray = 0.920000017\n",
      "Epoch: 0004 cost = 0.415605340 accuray = 0.939999998\n",
      "Epoch: 0005 cost = 0.414478996 accuray = 0.829999983\n",
      "Epoch: 0006 cost = 0.428242233 accuray = 0.879999995\n",
      "Epoch: 0007 cost = 0.473806663 accuray = 0.879999995\n",
      "Epoch: 0008 cost = 0.434526913 accuray = 0.879999995\n",
      "Epoch: 0009 cost = 0.436063575 accuray = 0.939999998\n",
      "Epoch: 0010 cost = 0.463994310 accuray = 0.879999995\n",
      "Epoch: 0011 cost = 0.478092829 accuray = 0.899999976\n",
      "Epoch: 0012 cost = 0.436143533 accuray = 0.920000017\n",
      "Epoch: 0013 cost = 0.435363550 accuray = 0.910000026\n",
      "Epoch: 0014 cost = 0.464855079 accuray = 0.910000026\n",
      "Epoch: 0015 cost = 0.462609173 accuray = 0.910000026\n",
      "Learning finished\n",
      "Accuracy:  0.9483\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZt43AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c0707c7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 6.학습 진행 Training cycle(15회 반복)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        #number of iterations\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "                \n",
    "        #1epoch \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _, a = sess.run([cost, optimizer, accuracy], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost),\n",
    "              'accuray =', '{:.9f}'.format(a))\n",
    "        \n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # 7. Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "    \n",
    "\n",
    "    #시각화\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Network Model Class\n",
    "    \n",
    "    Note that this class has only the constructor.\n",
    "    The actual model is defined inside the constructor.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    X : tf.float32\n",
    "        This is a tensorflow placeholder for MNIST images\n",
    "        Expected shape is [None, 784]\n",
    "        \n",
    "    y : tf.float32\n",
    "        This is a tensorflow placeholder for MNIST labels (one hot encoded)\n",
    "        Expected shape is [None, 10]\n",
    "        \n",
    "    mode : tf.bool\n",
    "        This is used for the batch normalization\n",
    "        It's `True` at training time and `False` at test time\n",
    "        \n",
    "    loss : tf.float32\n",
    "        The loss function is a softmax cross entropy\n",
    "        \n",
    "    train_op\n",
    "        This is simply the training op that minimizes the loss\n",
    "        \n",
    "    accuracy : tf.float32\n",
    "        The accuracy operation\n",
    "        \n",
    "    \n",
    "    Examples\n",
    "    ----------\n",
    "    >>> model = Model(\"Batch Norm\", 32, 10)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n",
    "        \"\"\" Constructor\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        name : str\n",
    "            The name of this network\n",
    "            The entire network will be created under `tf.variable_scope(name)`\n",
    "            \n",
    "        input_dim : int\n",
    "            The input dimension\n",
    "            In this example, 784\n",
    "        \n",
    "        output_dim : int\n",
    "            The number of output labels\n",
    "            There are 10 labels\n",
    "            \n",
    "        hidden_dims : list (default: [32, 32])\n",
    "            len(hidden_dims) = number of layers\n",
    "            each element is the number of hidden units\n",
    "            \n",
    "        use_batchnorm : bool (default: True)\n",
    "            If true, it will create the batchnormalization layer\n",
    "            \n",
    "        activation_fn : TF functions (default: tf.nn.relu)\n",
    "            Activation Function\n",
    "            \n",
    "        optimizer : TF optimizer (default: tf.train.AdamOptimizer)\n",
    "            Optimizer Function\n",
    "            \n",
    "        lr : float (default: 0.01)\n",
    "            Learning rate\n",
    "        \n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            # Placeholders are defined\n",
    "            self.X = tf.placeholder(tf.float32, [None, input_dim], name='X')\n",
    "            self.y = tf.placeholder(tf.float32, [None, output_dim], name='y')\n",
    "            self.mode = tf.placeholder(tf.bool, name='train_mode')            \n",
    "            \n",
    "            # Loop over hidden layers\n",
    "            net = self.X\n",
    "            for i, h_dim in enumerate(hidden_dims):\n",
    "                with tf.variable_scope('layer{}'.format(i)):\n",
    "                    net = tf.layers.dense(net, h_dim)\n",
    "                    \n",
    "                    if use_batchnorm:\n",
    "                        net = tf.layers.batch_normalization(net, training=self.mode)\n",
    "                        \n",
    "                    net = activation_fn(net)\n",
    "            \n",
    "            # Attach fully connected layers\n",
    "            net = tf.contrib.layers.flatten(net)\n",
    "            net = tf.layers.dense(net, output_dim)\n",
    "            \n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.loss, name='loss')    \n",
    "            \n",
    "            # When using the batchnormalization layers,\n",
    "            # it is necessary to manually add the update operations\n",
    "            # because the moving averages are not included in the graph            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "            with tf.control_dependencies(update_ops):                     \n",
    "                self.train_op = optimizer(lr).minimize(self.loss)\n",
    "            \n",
    "            # Accuracy etc \n",
    "            softmax = tf.nn.softmax(net, name='softmax')\n",
    "            self.accuracy = tf.equal(tf.argmax(softmax, 1), tf.argmax(self.y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.accuracy, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    \"\"\"Solver class\n",
    "    \n",
    "    This class will contain the model class and session\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    model : Model class\n",
    "    sess : TF session\n",
    "        \n",
    "    Methods\n",
    "    ----------\n",
    "    train(X, y)\n",
    "        Run the train_op and Returns the loss\n",
    "        \n",
    "    evalulate(X, y, batch_size=None)\n",
    "        Returns \"Loss\" and \"Accuracy\"\n",
    "        If batch_size is given, it's computed using batch_size\n",
    "        because most GPU memories cannot handle the entire training data at once\n",
    "            \n",
    "    Example\n",
    "    ----------\n",
    "    >>> sess = tf.InteractiveSession()\n",
    "    >>> model = Model(\"BatchNorm\", 32, 10)\n",
    "    >>> solver = Solver(sess, model)\n",
    "    \n",
    "    # Train\n",
    "    >>> solver.train(X, y)\n",
    "    \n",
    "    # Evaluate\n",
    "    >>> solver.evaluate(X, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, model):\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        feed = {\n",
    "            self.model.X: X,\n",
    "            self.model.y: y,\n",
    "            self.model.mode: True\n",
    "        }\n",
    "        train_op = self.model.train_op\n",
    "        loss = self.model.loss\n",
    "        \n",
    "        return self.sess.run([train_op, loss], feed_dict=feed)\n",
    "    \n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        if batch_size:\n",
    "            N = X.shape[0]\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            \n",
    "            for i in range(0, N, batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                \n",
    "                feed = {\n",
    "                    self.model.X: X_batch,\n",
    "                    self.model.y: y_batch,\n",
    "                    self.model.mode: False\n",
    "                }\n",
    "                \n",
    "                loss = self.model.loss\n",
    "                accuracy = self.model.accuracy\n",
    "                \n",
    "                step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed)\n",
    "                \n",
    "                total_loss += step_loss * X_batch.shape[0]\n",
    "                total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "            total_loss /= N\n",
    "            total_acc /= N\n",
    "            \n",
    "            return total_loss, total_acc\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            feed = {\n",
    "                self.model.X: X,\n",
    "                self.model.y: y,\n",
    "                self.model.mode: False\n",
    "            }\n",
    "            \n",
    "            loss = self.model.loss            \n",
    "            accuracy = self.model.accuracy\n",
    "\n",
    "            return self.sess.run([loss, accuracy], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "output_dim = 10\n",
    "N = 55000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# We create two models: one with the batch norm and other without\n",
    "bn = Model('batchnorm', input_dim, output_dim, use_batchnorm=True)\n",
    "nn = Model('no_norm', input_dim, output_dim, use_batchnorm=False)\n",
    "\n",
    "# We create two solvers: to train both models at the same time for comparison\n",
    "# Usually we only need one solver class\n",
    "bn_solver = Solver(sess, bn)\n",
    "nn_solver = Solver(sess, nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_n = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Save Losses and Accuracies every epoch\n",
    "# We are going to plot them later\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0-TRAIN] Batchnorm Loss(Acc): 0.17398(94.64%) vs No Batchnorm Loss(Acc): 0.18036(94.59%)\n",
      "[Epoch 0-VALID] Batchnorm Loss(Acc): 0.19834(94.30%) vs No Batchnorm Loss(Acc): 0.19411(94.34%)\n",
      "\n",
      "[Epoch 1-TRAIN] Batchnorm Loss(Acc): 0.10822(96.55%) vs No Batchnorm Loss(Acc): 0.16093(95.23%)\n",
      "[Epoch 1-VALID] Batchnorm Loss(Acc): 0.12686(96.18%) vs No Batchnorm Loss(Acc): 0.17837(95.22%)\n",
      "\n",
      "[Epoch 2-TRAIN] Batchnorm Loss(Acc): 0.08982(97.24%) vs No Batchnorm Loss(Acc): 0.16440(95.29%)\n",
      "[Epoch 2-VALID] Batchnorm Loss(Acc): 0.11797(96.62%) vs No Batchnorm Loss(Acc): 0.19405(95.16%)\n",
      "\n",
      "[Epoch 3-TRAIN] Batchnorm Loss(Acc): 0.08890(97.17%) vs No Batchnorm Loss(Acc): 0.16203(95.41%)\n",
      "[Epoch 3-VALID] Batchnorm Loss(Acc): 0.11283(96.62%) vs No Batchnorm Loss(Acc): 0.19755(95.14%)\n",
      "\n",
      "[Epoch 4-TRAIN] Batchnorm Loss(Acc): 0.07484(97.64%) vs No Batchnorm Loss(Acc): 0.16005(95.37%)\n",
      "[Epoch 4-VALID] Batchnorm Loss(Acc): 0.11221(96.80%) vs No Batchnorm Loss(Acc): 0.19260(94.98%)\n",
      "\n",
      "[Epoch 5-TRAIN] Batchnorm Loss(Acc): 0.05870(98.14%) vs No Batchnorm Loss(Acc): 0.13650(96.12%)\n",
      "[Epoch 5-VALID] Batchnorm Loss(Acc): 0.10577(97.02%) vs No Batchnorm Loss(Acc): 0.19746(95.02%)\n",
      "\n",
      "[Epoch 6-TRAIN] Batchnorm Loss(Acc): 0.06294(98.01%) vs No Batchnorm Loss(Acc): 0.12939(96.38%)\n",
      "[Epoch 6-VALID] Batchnorm Loss(Acc): 0.10344(96.98%) vs No Batchnorm Loss(Acc): 0.18039(95.96%)\n",
      "\n",
      "[Epoch 7-TRAIN] Batchnorm Loss(Acc): 0.05749(98.09%) vs No Batchnorm Loss(Acc): 0.12667(96.37%)\n",
      "[Epoch 7-VALID] Batchnorm Loss(Acc): 0.09228(97.22%) vs No Batchnorm Loss(Acc): 0.16588(95.74%)\n",
      "\n",
      "[Epoch 8-TRAIN] Batchnorm Loss(Acc): 0.05096(98.35%) vs No Batchnorm Loss(Acc): 0.14275(96.15%)\n",
      "[Epoch 8-VALID] Batchnorm Loss(Acc): 0.09340(97.40%) vs No Batchnorm Loss(Acc): 0.20280(95.36%)\n",
      "\n",
      "[Epoch 9-TRAIN] Batchnorm Loss(Acc): 0.05523(98.17%) vs No Batchnorm Loss(Acc): 0.12551(96.41%)\n",
      "[Epoch 9-VALID] Batchnorm Loss(Acc): 0.09909(97.20%) vs No Batchnorm Loss(Acc): 0.17647(95.44%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    for _ in range(N//batch_size):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, bn_loss = bn_solver.train(X_batch, y_batch)\n",
    "        _, nn_loss = nn_solver.train(X_batch, y_batch)       \n",
    "    \n",
    "    b_loss, b_acc = bn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    n_loss, n_acc = nn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    \n",
    "    # Save train losses/acc\n",
    "    train_losses.append([b_loss, n_loss])\n",
    "    train_accs.append([b_acc, n_acc])\n",
    "    print(f'[Epoch {epoch}-TRAIN] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    \n",
    "    b_loss, b_acc = bn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    n_loss, n_acc = nn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    \n",
    "    # Save valid losses/acc\n",
    "    valid_losses.append([b_loss, n_loss])\n",
    "    valid_accs.append([b_acc, n_acc])\n",
    "    print(f'[Epoch {epoch}-VALID] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_solver.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_solver.evaluate(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_compare(loss_list: list, ylim=None, title=None) -> None:\n",
    "    \n",
    "    bn = [i[0] for i in loss_list]\n",
    "    nn = [i[1] for i in loss_list]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(bn, label='With BN')\n",
    "    plt.plot(nn, label='Without BN')\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "        \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_compare(train_losses, title='Training Loss at Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2-3. Updated MNIST(Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 28\n",
    "height = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. 데이터 생성하기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, width*height).astype('float32')\n",
    "x_test = x_test.reshape(10000, width*height).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#훈련 셋과 검증 셋 분리\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=width*height, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 모델 학습과정 설정하기\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      " 7616/50000 [===>..........................] - ETA: 1:44 - loss: 14.4144 - acc: 0.1057"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-6b9e354b46d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#4. 모델 학습시키기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-4919109fcdb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0macc_ax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwinx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'var_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0de050df28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['var_loss'], 'y', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "acc_ax.plot(hist.history(['acc'], 'b', label='train acc'))\n",
    "acc_ax.plot(hist.history(['val_acc'], 'g', label='val acc'))\n",
    "acc_ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 516us/step\n",
      "# evaluation loss_and_metrics\n",
      "[14.490167601013184, 0.10100000000000001]\n"
     ]
    }
   ],
   "source": [
    "#6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(\"# evaluation loss_and_metrics\")\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-97a5828333a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#7. 모델 사용하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myhat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1748\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row=0.5\n",
    "plt_col=0.5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "cnt=0\n",
    "i=0\n",
    "\n",
    "while cnt < (plt_row*plt_col):\n",
    "    if np.argmax(y_test[i]) == np.argmax(yhat_test[i]):\n",
    "        i+=1\n",
    "        continue\n",
    "    \n",
    "    sub_plt = ararr[cnt/plt_row, cnt%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    sub_plt_title = 'R: ' + str(np.argmax(y_test[i])) + 'P: ' + str(np.argmax(yhat_test[i]))\n",
    "    \n",
    "    i+=1\n",
    "    cnt+=1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
